{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vD7G3tXsH7k0",
    "outputId": "8cdd0de6-fec2-4db3-8da2-cba0a1f6af4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lorem in ./anaconda3/lib/python3.10/site-packages (0.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrL-AdL7GHDY"
   },
   "source": [
    "Create sample text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MX1DHh4nGMZY"
   },
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "\n",
    "with open(\"sample.txt\", \"w\") as f:\n",
    "    for i in range(2):\n",
    "        f.write(text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CHHnCN_bIStu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porro quisquam est dolore velit voluptatem magnam non. Amet numquam quiquia magnam labore non aliquam etincidunt. Quisquam eius sed neque amet quaerat adipisci dolor. Ut labore velit non. Quiquia sed tempora adipisci numquam dolorem sed. Numquam etincidunt sed modi ut dolore neque est. Consectetur numquam voluptatem numquam. Sed ut labore aliquam aliquam ut sed ut.\r\n",
      "\r\n",
      "Amet dolor modi amet. Eius dolorem sed sit quaerat dolore. Adipisci velit magnam dolorem amet labore tempora adipisci. Neque ut quaerat ipsum quaerat ut modi est. Numquam ipsum non consectetur sit.\r\n",
      "\r\n",
      "Labore dolorem neque magnam modi voluptatem. Adipisci quiquia dolor velit ut. Eius etincidunt labore aliquam eius quisquam amet quisquam. Voluptatem ut ipsum dolor numquam ut velit. Modi ipsum ipsum sed non labore. Tempora sed voluptatem dolor velit ut. Amet voluptatem numquam velit eius adipisci adipisci. Ipsum tempora quisquam adipisci non non etincidunt dolorem.Adipisci non tempora eius. Numquam dolore ut amet sit. Neque ipsum sed modi modi neque adipisci eius. Porro consectetur eius porro. Ut aliquam neque voluptatem. Voluptatem porro neque sit dolore sit. Porro aliquam dolorem sit tempora consectetur quisquam sit. Dolorem quaerat voluptatem aliquam est etincidunt.\r\n",
      "\r\n",
      "Aliquam velit ut dolorem quaerat. Ipsum quiquia dolore sed est sit. Quisquam quisquam sed porro dolore sed consectetur. Dolorem ut est sed ut adipisci amet. Quaerat sit eius est dolor voluptatem adipisci adipisci. Dolor non aliquam numquam numquam non. Non tempora magnam neque. Adipisci quiquia dolor ipsum voluptatem magnam. Amet labore labore tempora eius eius.\r\n",
      "\r\n",
      "Quaerat amet quaerat aliquam eius numquam magnam. Voluptatem adipisci ut quaerat dolorem sit voluptatem dolor. Quisquam etincidunt voluptatem dolor. Labore dolore neque dolore modi. Porro dolorem consectetur neque consectetur. Velit velit aliquam ipsum dolore velit. Neque est magnam labore porro.\r\n",
      "\r\n",
      "Numquam quiquia non tempora voluptatem aliquam. Sed quiquia quiquia adipisci. Ut ipsum adipisci velit quaerat eius est. Ut dolore sed dolorem quiquia dolorem. Labore amet quaerat adipisci sit. Etincidunt aliquam eius aliquam sit neque. Numquam porro numquam quaerat quisquam magnam tempora magnam."
     ]
    }
   ],
   "source": [
    "cat sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssgz69-AGGjF"
   },
   "source": [
    "Write a python program that counts the number of lines, different words and characters in that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dYPhX2nWGStx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      10     320    2209 sample.txt\n",
      "4.0K\tsample.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wc sample.txt\n",
    "du -h sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hipH17MhGV7l"
   },
   "source": [
    "Exercise 4.2\n",
    "Create a function called map_words that take a file name as argument and return a lists containing all words as items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pkAECi__GY0_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Porro', 'quisquam', 'est', 'dolore', 'velit']\n"
     ]
    }
   ],
   "source": [
    "#defining a \n",
    "def map_words(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        words = file.read().split()\n",
    "    return words\n",
    "\n",
    "map_words(\"sample.txt\")[:5] # first five words\n",
    "word_list = map_words(\"sample.txt\")[:5]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WecyaZ07GcCQ"
   },
   "source": [
    "Sorting a dictionary by value\n",
    "By default, if you use sorted function on a dict, it will use keys to sort it. To sort by values, you can use operator.itemgetter(1) Return a callable object that fetches item from its operand using the operand’s __getitem__( method. It could be used to sort results.\n",
    "sorted function has also a reverse optional argument.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "htkvNwTKGiqq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pear': 5, 'apple': 3, 'banana': 2, 'orange': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "fruits = [('apple', 3), ('banana', 2), ('pear', 5), ('orange', 1)]\n",
    "getcount = operator.itemgetter(1)\n",
    "dict(sorted(fruits, key=getcount))\n",
    "\n",
    "dict(sorted(fruits, key=getcount, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xkGhhR3Gmsx"
   },
   "source": [
    "Exercise 4.3\n",
    "Create a function reduce to reduce the list of words returned by map_words and return a dictionary containing all words as keys and number of occurrences as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nfLpc8k_GvGN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Quiquia': 1, 'sed.': 1, 'Consectetur': 1, 'numquam.': 1, 'dolore.': 1, 'quisquam.': 1, 'Modi': 1, 'labore.': 1, 'Tempora': 1, 'dolorem.Adipisci': 1, 'Aliquam': 1, 'quaerat.': 1, 'Dolor': 1, 'Non': 1, 'modi.': 1, 'Velit': 1, 'aliquam.': 1, 'dolorem.': 1, 'Etincidunt': 1, 'etincidunt.': 2, 'Sed': 2, 'amet.': 2, 'Eius': 2, 'voluptatem.': 2, 'velit.': 2, 'Ipsum': 2, 'porro.': 2, 'Dolorem': 2, 'consectetur.': 2, 'Quaerat': 2, 'neque.': 2, 'non.': 3, 'Quisquam': 3, 'dolor.': 3, 'est.': 3, 'ut.': 3, 'Adipisci': 3, 'Neque': 3, 'Labore': 3, 'Voluptatem': 3, 'eius.': 3, 'porro': 3, 'magnam.': 3, 'Porro': 4, 'Amet': 4, 'Ut': 4, 'etincidunt': 4, 'adipisci.': 4, 'consectetur': 4, 'Numquam': 5, 'quisquam': 6, 'est': 6, 'amet': 6, 'modi': 6, 'dolor': 6, 'sit': 6, 'sit.': 6, 'magnam': 7, 'quiquia': 8, 'labore': 8, 'non': 8, 'neque': 8, 'velit': 9, 'numquam': 9, 'eius': 9, 'tempora': 9, 'dolorem': 9, 'ipsum': 9, 'dolore': 10, 'voluptatem': 10, 'quaerat': 10, 'adipisci': 10, 'aliquam': 12, 'ut': 12, 'sed': 13}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def reduce(file_name):\n",
    "    words = map_words(file_name)\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word in word_count:\n",
    "            word_count[word] += 1\n",
    "        else:\n",
    "            word_count[word] = 1\n",
    "    \n",
    "    sorted_word_count = sorted(word_count.items(), \n",
    "                               key=operator.itemgetter(1))\n",
    "    sorted_dict = dict(sorted_word_count)\n",
    "    return sorted_dict\n",
    "\n",
    "word_count_dict = reduce(\"sample.txt\")\n",
    "print(word_count_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGhMka6eGv39"
   },
   "source": [
    "You probably notice that this simple function is not easy to implement. Python standard library provides some features that can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uagp-nzlGzpr"
   },
   "source": [
    "Container datatypes\n",
    "collection module implements specialized container datatypes providing alternatives to Python’s general purpose built-in containers, dict, list, set, and tuple.\n",
    "\n",
    "defaultdict : dict subclass that calls a factory function to supply missing values\n",
    "\n",
    "Counter : dict subclass for counting hashable objects\n",
    "\n",
    "defaultdict\n",
    "When you implement the wordcount function you probably had some problem to append key-value pair to your dict. If you try to change the value of a key that is not present in the dict, the key is not automatically created.\n",
    "\n",
    "You can use a try-except flow but the defaultdict could be a solution. This container is a dict subclass that calls a factory function to supply missing values. For example, using list as the default_factory, it is easy to group a sequence of key-value pairs into a dictionary of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Gk0muiVsHECO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n",
    "d = defaultdict(list)\n",
    "for k, v in s:\n",
    "    d[k].append(v)\n",
    "\n",
    "dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et-AGlSCHJXe"
   },
   "source": [
    "Modify the reduce function you wrote above by using a defaultdict with the most suitable factory.Counter\n",
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts.\n",
    "\n",
    "Elements are counted from an iterable or initialized from another mapping (or counter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CIGs5QgQHSgB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 23, 'g': 13, 'b': 23}\n",
      "0\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "violet = dict(r=23,g=13,b=23)\n",
    "print(violet)\n",
    "cnt = Counter(violet)  # or Counter(r=238, g=130, b=238)\n",
    "print(cnt['c'])\n",
    "print(cnt['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K0A9ePWMHVcw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r r r r r r r r r r r r r r r r r r r r r r r g g g g g g g g g g g g g b b b b b b b b b b b b b b b b b b b b b b b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_values([23, 13, 23])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(*cnt.elements())\n",
    "cnt.most_common(2)\n",
    "cnt.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1xCn1wXHeOz"
   },
   "source": [
    "Exercise 4.5\n",
    "Use a Counter object to count words occurences in the sample text file.\n",
    "\n",
    "The Counter class is similar to bags or multisets in some Python libraries or other languages. We will see later how to use Counter-like objects in a parallel context.\n",
    "\n",
    "Process multiple files\n",
    "Create several files containing lorem text named ‘sample01.txt’, ‘sample02.txt’…\n",
    "\n",
    "If you process these files you return multiple dictionaries.\n",
    "\n",
    "You have to loop over them to sum occurences and return the resulted dict. To iterate on specific mappings, Python standard library provides some useful features in itertools module.\n",
    "\n",
    "itertools.chain(*mapped_values) could be used for treating consecutive sequences as a single sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'velit': 22, 'aliquam': 18, 'eius': 17, 'tempora': 17, 'ut': 16, 'quisquam': 16, 'numquam': 16, 'magnam': 15, 'sed': 15, 'etincidunt': 14, 'quiquia': 14, 'est': 14, 'amet': 14, 'porro': 12, 'dolore': 12, 'neque': 12, 'quaerat': 11, 'modi': 11, 'sit': 11, 'non': 11, 'labore': 11, 'dolorem': 11, 'consectetur': 11, 'adipisci': 10, 'voluptatem': 10, 'ipsum': 10, 'dolor': 9, 'Amet': 9, 'eius.': 8, 'Est': 8, 'Adipisci': 8, 'non.': 6, 'sit.': 6, 'Consectetur': 6, 'ipsum.': 6, 'ut.': 5, 'etincidunt.': 5, 'Numquam': 5, 'Sed': 5, 'sed.': 5, 'dolorem.': 5, 'Aliquam': 4, 'amet.': 4, 'est.': 4, 'Velit': 4, 'Tempora': 4, 'dolor.': 4, 'Etincidunt': 3, 'magnam.': 3, 'Porro': 3, 'Magnam': 3, 'quiquia.': 3, 'quisquam.': 3, 'Voluptatem': 3, 'velit.': 3, 'porro.': 3, 'tempora.': 3, 'voluptatem.': 3, 'Neque': 3, 'adipisci.': 3, 'modi.': 3, 'neque.': 3, 'Sit': 2, 'Dolore': 2, 'Quiquia': 2, 'Non': 2, 'Ipsum': 2, 'Modi': 2, 'Quaerat': 2, 'Eius': 2, 'Dolor': 2, 'Ut': 2, 'dolore.': 2, 'Dolorem': 2, 'Labore': 2, 'Quisquam': 1, 'consectetur.': 1, 'quaerat.': 1, 'labore.': 1})\n"
     ]
    }
   ],
   "source": [
    " from collections import Counter\n",
    "\n",
    "def map_words(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        words = file.read().split()\n",
    "    return words\n",
    "\n",
    "def reduce(file_names):\n",
    "    word_count = Counter()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        words = map_words(file_name)\n",
    "        word_count += Counter(words)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "file_names = ['sample01.txt', 'sample02.txt', 'sample03.txt']\n",
    "\n",
    "# Create files\n",
    "for file_name in file_names:\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(text())\n",
    "\n",
    "combined_word_count = reduce(file_names)\n",
    "print(combined_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this code, we first define the map_words function, which extracts words from a file. The reduce function is created to process multiple files and count word occurrences using the Counter object. Next, we create the sample text files by looping over the file_names list and writing the generated text into each file using the text() function. Then, we call the reduce function with the file_names list to process and count the word occurrences in the files. The resulting combined word count dictionary is stored in the combined_word_count variable. Finally, we print the combined word count dictionary. Please ensure that you have the necessary import statements and the text() function is generating the desired random text for the sample files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iChrtoNbHfEN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orange': 1,\n",
       " 'spinach': 1,\n",
       " 'banana': 2,\n",
       " 'endive': 2,\n",
       " 'apple': 3,\n",
       " 'carrot': 4,\n",
       " 'pear': 5,\n",
       " 'celery': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools, operator\n",
    "fruits = [('apple', 3), ('banana', 2), ('pear', 5), ('orange', 1)]\n",
    "vegetables = [('endive', 2), ('spinach', 1), ('celery', 5), ('carrot', 4)]\n",
    "getcount = operator.itemgetter(1)\n",
    "dict(sorted(itertools.chain(fruits,vegetables), key=getcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rvJIU_EHhVR"
   },
   "source": [
    "Exercise 4.6\n",
    "Write the program that creates files, processes and use itertools.chain to get the merged word count dictionary.\n",
    "\n",
    "Exercise 4.7\n",
    "Create the wordcount function in order to accept several files as arguments and return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "i6BZUmcJHjx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Word Count:\n",
      "Counter({'Hey': 3, 'My': 3, 'name': 3, 'is': 3, 'Pritika': 3, '1.': 1, '2.': 1, '3.': 1})\n",
      "Merged Word Count using wordcount function:\n",
      "Counter({'Hey': 3, 'My': 3, 'name': 3, 'is': 3, 'Pritika': 3, '1.': 1, '2.': 1, '3.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def map_words(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        words = file.read().split()\n",
    "    return words\n",
    "\n",
    "def wordcount(*file_names):\n",
    "    word_count = Counter()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        words = map_words(file_name)\n",
    "        word_count += Counter(words)\n",
    "\n",
    "    return word_count\n",
    "\n",
    "file_names = ['sample01.txt', 'sample02.txt', 'sample03.txt']\n",
    "\n",
    "# Create files\n",
    "for i, file_name in enumerate(file_names):\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(f\"Hey My name is Pritika {i+1}.\")\n",
    "\n",
    "merged_word_count = Counter()\n",
    "for file_name in file_names:\n",
    "    words = map_words(file_name)\n",
    "    merged_word_count += Counter(words)\n",
    "\n",
    "print(\"Merged Word Count:\")\n",
    "print(merged_word_count)\n",
    "\n",
    "merged_word_count = wordcount(*file_names)\n",
    "\n",
    "print(\"Merged Word Count using wordcount function:\")\n",
    "print(merged_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhl4t_eBHnr0"
   },
   "source": [
    "Hint: arbitrary argument lists\n",
    "\n",
    "Example of use of arbitrary argument list and arbitrary named arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7xDDjXZzHqQH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[1, 2]\n",
      "bonjour\n",
      "{'x': 4, 'y': 'y'}\n"
     ]
    }
   ],
   "source": [
    "def func( *args, **kwargs):\n",
    "    for arg in args:\n",
    "        print(arg)\n",
    "\n",
    "    print(kwargs)\n",
    "\n",
    "func( \"3\", [1,2], \"bonjour\", x = 4, y = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReBvZNOtd-rL",
    "outputId": "c1bf48ea-fd17-48cc-b90d-8bb907cceae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -24 -15 21\n"
     ]
    }
   ],
   "source": [
    "from operator import mul\n",
    "rdd1, rdd2 = [2, 6, -3, 7], [1, -4, 5, 3]\n",
    "res = map(mul, rdd1, rdd2 )\n",
    "print(*res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIokKVUbeNam",
    "outputId": "6dbbc8f7-bb85-474f-9959-d35cbf0d7e5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "rdd = list(range(1,6))\n",
    "reduce(add, rdd) # computes ((((1+2)+3)+4)+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o71qT3D7e31i",
    "outputId": "0ae4a242-2ade-4542-b1b9-7b4799f7361e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hey', 3), ('My', 3), ('name', 3), ('is', 3), ('Pritika', 3), ('1.', 1), ('2.', 1), ('3.', 1)]\n"
     ]
    }
   ],
   "source": [
    "from lorem import text\n",
    "\n",
    "def mapper(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        contents = file.read()\n",
    "        words = contents.split()  # Corrected: Added parentheses after split\n",
    "        word_tuples = [(word, 1) for word in words]\n",
    "        return word_tuples\n",
    "\n",
    "def partitioner(mapper_output):\n",
    "    partitioned_data = {}\n",
    "    for word, value in mapper_output:\n",
    "        if word in partitioned_data:\n",
    "            partitioned_data[word].append(value)\n",
    "        else:\n",
    "            partitioned_data[word] = [value]\n",
    "    partitioned_list = [(word, values) for word, values in partitioned_data.items()]\n",
    "    return partitioned_list\n",
    "\n",
    "def reducer(word_tuple):\n",
    "    word, occurrences_list = word_tuple\n",
    "    count = sum(occurrences_list)\n",
    "    return (word, count)\n",
    "\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob('sample0*.txt'))\n",
    "mapper_output = map(mapper, files)\n",
    "mapper_output = list(mapper_output)  # Convert map object to a list\n",
    "partitioned_data = partitioner([item for sublist in mapper_output for item in sublist])\n",
    "word_counts = map(reducer, partitioned_data)\n",
    "\n",
    "print(list(word_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapreduce\n",
    "from time import sleep\n",
    "def f(x):\n",
    "    sleep(1)\n",
    "    return x*x\n",
    "L = list(range(8))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.94 ms, sys: 2.64 ms, total: 5.58 ms\n",
      "Wall time: 8.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum(f(x) for x in L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.48 ms, sys: 1.86 ms, total: 4.34 ms\n",
      "Wall time: 8.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum(map(f,L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing\n",
    "\n",
    "#### multiprocessing is a package that supports spawning processes. We can use it to display how many concurrent processes you can launch on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def main():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print(f\"Number of CPUs: {num_cpus}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pmap.py\n"
     ]
    }
   ],
   "source": [
    "%%file pmap.py\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from time import sleep, time\n",
    "\n",
    "def f(x):\n",
    "    sleep(1)\n",
    "    return x*x\n",
    "\n",
    "L = list(range(8))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    begin = time()\n",
    "    with ProcessPoolExecutor() as pool:\n",
    "\n",
    "        result = sum(pool.map(f, L))\n",
    "    end = time()\n",
    "    \n",
    "    print(f\"result = {result} and time = {end-begin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = 140 and time = 1.1987218856811523\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} pmap.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProcessPoolExecutor launches one slave process per physical core on the computer. pool.map divides the input list into chunks and puts the tasks (function + chunk) on a queue. Each slave process takes a task (function + a chunk of data), runs map(function, chunk), and puts the result on a result list. pool.map on the master process waits until all tasks are handled and returns the concatenation of the result lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "CPU times: user 5.21 ms, sys: 4.1 ms, total: 9.3 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "\n",
    "    results = sum(pool.map(f, L))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread and Process: \n",
    "#### Differences. A process is an instance of a running program.\n",
    "#### Process may contain one or more threads, but a thread cannot contain a process.\n",
    "#### Process has a self-contained execution environment. It has its own memory space.\n",
    "#### Application running on your computer may be a set of cooperating processes.\n",
    "#### Process don't share its memory, communication between processes implies data serialization.\n",
    "#### A thread is made of and exist within a process; every process has at least one thread.\n",
    "#### Multiple threads in a process share resources, which helps in efficient communication between threads.\n",
    "#### Threads can be concurrent on a multi-core system, with every core executing the separate threads simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelize text files downloads\n",
    "Victor Hugo http://www.gutenberg.org/files/135/135-0.txt\n",
    "Marcel Proust http://www.gutenberg.org/files/7178/7178-8.txt\n",
    "Emile Zola http://www.gutenberg.org/files/1069/1069-0.txt\n",
    "Stendhal http://www.gutenberg.org/files/44747/44747-0.txt\n",
    "\n",
    "\n",
    "### Exercise 6.1\n",
    "#### Use ThreadPoolExecutor to parallelize the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download books/Marcel Proust.txt\n",
      "books/Stendhal.txt downloaded successfully.\n",
      "books/Emile Zola.txt downloaded successfully.\n",
      "books/Victor Hugo.txt downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_text(url, filename):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"{filename} downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}\")\n",
    "\n",
    "def main():\n",
    "    urls = {\n",
    "        \"Victor Hugo\": \"http://www.gutenberg.org/files/135/135-0.txt\",\n",
    "        \"Marcel Proust\": \"http://www.gutenberg.org/files/7178/7178-8.txt\",\n",
    "        \"Emile Zola\": \"http://www.gutenberg.org/files/1069/1069-0.txt\",\n",
    "        \"Stendhal\": \"http://www.gutenberg.org/files/44747/44747-0.txt\"\n",
    "    }\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for name, url in urls.items():\n",
    "            filename = os.path.join(\"books\", f\"{name}.txt\")\n",
    "            future = executor.submit(download_text, url, filename)\n",
    "            futures.append(future)\n",
    "\n",
    "        # Wait for all the downloads to complete\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def mapper(filename):\n",
    "    \" split text to list of key/value pairs (word,1)\"\n",
    "\n",
    "    with open(filename) as f:\n",
    "        data = f.read()\n",
    "        \n",
    "    data = data.strip().replace(\".\",\"\").lower().split()\n",
    "        \n",
    "    return sorted([(w,1) for w in data])\n",
    "\n",
    "def partitioner(mapped_values):\n",
    "    \"\"\" get lists from mapper and create a dict with\n",
    "    (word,[1,1,1])\"\"\"\n",
    "    \n",
    "    res = defaultdict(list)\n",
    "    for w, c in mapped_values:\n",
    "        res[w].append(c)\n",
    "        \n",
    "    return res.items()\n",
    "\n",
    "def reducer( item ):\n",
    "    \"\"\" Compute words occurences from dict computed\n",
    "    by partioner\n",
    "    \"\"\"\n",
    "    w, v = item\n",
    "    return (w,len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathos in ./anaconda3/lib/python3.10/site-packages (0.3.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in ./anaconda3/lib/python3.10/site-packages (from pathos) (0.70.15)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in ./anaconda3/lib/python3.10/site-packages (from pathos) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in ./anaconda3/lib/python3.10/site-packages (from pathos) (0.3.3)\n",
      "Requirement already satisfied: dill>=0.3.7 in ./anaconda3/lib/python3.10/site-packages (from pathos) (0.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pathos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pritika_timsina/anaconda3/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/pritika_timsina/anaconda3/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/pritika_timsina/anaconda3/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/Users/pritika_timsina/anaconda3/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'mapper' on <module '__main__' (built-in)>\n",
      "Process SpawnProcess-2:\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Using concurrent.futures.ProcessPoolExecutor for mapping\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 34\u001b[0m     mapper_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m partitioned_data \u001b[38;5;241m=\u001b[39m partitioner([item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m mapper_output \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Using concurrent.futures.ProcessPoolExecutor for reducing\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/process.py:570\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    571\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def mapper(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            contents = file.read()\n",
    "            words = contents.split()\n",
    "            word_tuples = [(word, 1) for word in words]\n",
    "        return word_tuples\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return []\n",
    "\n",
    "def partitioner(mapper_output):\n",
    "    partitioned_data = {}\n",
    "    for word, value in mapper_output:\n",
    "        if word in partitioned_data:\n",
    "            partitioned_data[word].append(value)\n",
    "        else:\n",
    "            partitioned_data[word] = [value]\n",
    "    partitioned_list = [(word, values) for word, values in partitioned_data.items()]\n",
    "    return partitioned_list\n",
    "\n",
    "def reducer(word_tuple):\n",
    "    word, occurrences_list = word_tuple\n",
    "    count = sum(occurrences_list)\n",
    "    return (word, count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = sorted(glob.glob('sample0*.txt'))\n",
    "    # Using concurrent.futures.ProcessPoolExecutor for mapping\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        mapper_output = list(executor.map(mapper, files))\n",
    "\n",
    "    partitioned_data = partitioner([item for sublist in mapper_output for item in sublist])\n",
    "\n",
    "    # Using concurrent.futures.ProcessPoolExecutor for reducing\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        word_counts = list(executor.map(reducer, partitioned_data))\n",
    "\n",
    "    print(word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
